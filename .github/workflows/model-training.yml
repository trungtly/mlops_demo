name: Model Training and Validation

on:
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      model_types:
        description: 'Model types to train (comma-separated)'
        required: false
        default: 'random_forest,xgboost,lightgbm'
      feature_selection:
        description: 'Enable feature selection'
        required: false
        default: 'true'
        type: boolean
      hyperparameter_tuning:
        description: 'Enable hyperparameter tuning'
        required: false
        default: 'false'
        type: boolean

jobs:
  train-model:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model: [random_forest, xgboost, lightgbm]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Create data directory
      run: mkdir -p data

    - name: Download sample dataset (mock)
      run: |
        # Create a small mock dataset for CI purposes
        python -c "
        import pandas as pd
        import numpy as np
        np.random.seed(42)
        n_samples = 1000
        n_fraud = 50
        
        # Generate mock credit card transaction data
        data = []
        for i in range(n_samples):
            is_fraud = i < n_fraud
            data.append({
                'Time': np.random.randint(0, 172800),
                'Amount': np.random.exponential(50) if not is_fraud else np.random.exponential(200),
                'Class': int(is_fraud)
            })
            
        # Add V1-V28 features (PCA components)
        for i in range(28):
            col_name = f'V{i+1}'
            if is_fraud:
                data[-1][col_name] = np.random.normal(0, 2)
            else:
                data[-1][col_name] = np.random.normal(0, 1)
                
        df = pd.DataFrame(data)
        df.to_csv('data/creditcard.csv', index=False)
        print(f'Generated mock dataset with {len(df)} samples')
        "

    - name: Train model
      run: |
        python scripts/train_model.py \
          --data-path data/creditcard.csv \
          --model-type ${{ matrix.model }} \
          --output-dir models/ \
          --experiment-name "ci-${{ matrix.model }}-${{ github.run_id }}" \
          ${{ github.event.inputs.feature_selection == 'true' && '--feature-selection' || '' }} \
          ${{ github.event.inputs.hyperparameter_tuning == 'true' && '--hyperparameter-tuning' || '' }}

    - name: Evaluate model
      run: |
        python scripts/evaluate_model.py \
          --model-path models/${{ matrix.model }}_model.joblib \
          --data-path data/creditcard.csv \
          --output-dir evaluation/ \
          --save-predictions

    - name: Upload model artifacts
      uses: actions/upload-artifact@v3
      with:
        name: model-${{ matrix.model }}-${{ github.run_id }}
        path: |
          models/
          evaluation/
        retention-days: 30

    - name: Upload metrics
      uses: actions/upload-artifact@v3
      with:
        name: metrics-${{ matrix.model }}-${{ github.run_id }}
        path: evaluation/metrics.json
        retention-days: 90

  model-comparison:
    runs-on: ubuntu-latest
    needs: train-model
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas matplotlib seaborn

    - name: Download all metrics
      uses: actions/download-artifact@v3
      with:
        pattern: metrics-*
        path: all-metrics/

    - name: Compare models
      run: |
        python -c "
        import pandas as pd
        import json
        import os
        import matplotlib.pyplot as plt
        
        metrics_data = []
        
        for root, dirs, files in os.walk('all-metrics'):
            for file in files:
                if file == 'metrics.json':
                    model_name = root.split('-')[1] if '-' in root else 'unknown'
                    with open(os.path.join(root, file), 'r') as f:
                        metrics = json.load(f)
                        metrics['model'] = model_name
                        metrics_data.append(metrics)
        
        if metrics_data:
            df = pd.DataFrame(metrics_data)
            print('Model Comparison Results:')
            print(df[['model', 'accuracy', 'precision', 'recall', 'f1', 'auc']].round(4))
            
            # Save comparison
            df.to_csv('model_comparison.csv', index=False)
        else:
            print('No metrics found for comparison')
        "

    - name: Upload comparison results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: model-comparison-${{ github.run_id }}
        path: model_comparison.csv
        retention-days: 90