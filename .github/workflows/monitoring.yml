name: Data Quality and Model Monitoring

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      check_data_drift:
        description: 'Check for data drift'
        required: false
        default: 'true'
        type: boolean
      check_model_performance:
        description: 'Check model performance'
        required: false
        default: 'true'
        type: boolean

jobs:
  data-quality:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Create mock production data
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        
        np.random.seed(42)
        n_samples = 500
        
        # Simulate production data with potential drift
        data = []
        for i in range(n_samples):
            # Simulate some data drift by changing distributions
            drift_factor = 1.2 if np.random.random() > 0.8 else 1.0
            
            data.append({
                'Time': np.random.randint(0, 172800),
                'Amount': np.random.exponential(60) * drift_factor,  # Slight drift in amount
                'Class': np.random.choice([0, 1], p=[0.998, 0.002])  # Realistic fraud rate
            })
            
            # Add V1-V28 features with potential drift
            for j in range(28):
                col_name = f'V{j+1}'
                base_val = np.random.normal(0, 1)
                data[-1][col_name] = base_val * drift_factor if j < 5 else base_val
                
        df = pd.DataFrame(data)
        df['timestamp'] = pd.date_range(
            end=datetime.now(), 
            periods=len(df), 
            freq='1H'
        )
        
        df.to_csv('data/production_data.csv', index=False)
        print(f'Generated production data with {len(df)} samples')
        "

    - name: Run data quality checks
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        from datetime import datetime
        
        # Load production data
        df = pd.read_csv('data/production_data.csv')
        
        print('=== DATA QUALITY REPORT ===')
        print(f'Dataset shape: {df.shape}')
        print(f'Date range: {df[\"timestamp\"].min()} to {df[\"timestamp\"].max()}')
        
        # Check for missing values
        missing_pct = (df.isnull().sum() / len(df) * 100).round(2)
        if missing_pct.sum() > 0:
            print('\\nâš ï¸  Missing values detected:')
            print(missing_pct[missing_pct > 0])
        else:
            print('\\nâœ… No missing values detected')
        
        # Check for duplicates
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            print(f'\\nâš ï¸  {duplicates} duplicate rows detected')
        else:
            print('\\nâœ… No duplicate rows detected')
        
        # Check feature distributions
        feature_stats = df.describe()
        print('\\nðŸ“Š Feature statistics:')
        print(feature_stats[['Amount', 'V1', 'V2', 'V3']].round(3))
        
        # Check class balance
        class_dist = df['Class'].value_counts(normalize=True)
        print('\\nðŸŽ¯ Class distribution:')
        print(f'Normal: {class_dist[0]:.1%}')
        print(f'Fraud: {class_dist[1]:.1%}')
        
        # Data quality score
        quality_score = 100
        if missing_pct.sum() > 5:
            quality_score -= 20
        if duplicates > len(df) * 0.01:
            quality_score -= 10
        if class_dist[1] < 0.001 or class_dist[1] > 0.1:
            quality_score -= 15
            
        print(f'\\nðŸ“ˆ Overall Data Quality Score: {quality_score}/100')
        
        # Save quality report
        with open('data_quality_report.txt', 'w') as f:
            f.write(f'Data Quality Report - {datetime.now()}\\n')
            f.write(f'Quality Score: {quality_score}/100\\n')
            f.write(f'Missing Values: {missing_pct.sum():.2f}%\\n')
            f.write(f'Duplicates: {duplicates}\\n')
            f.write(f'Fraud Rate: {class_dist[1]:.3%}\\n')
        "

    - name: Check data drift
      if: ${{ github.event.inputs.check_data_drift == 'true' || github.event.inputs.check_data_drift == '' }}
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        from scipy import stats
        
        # Load reference data (training data)
        print('=== DATA DRIFT ANALYSIS ===')
        
        # Create mock reference data
        np.random.seed(123)
        n_ref = 1000
        ref_data = []
        for i in range(n_ref):
            ref_data.append({
                'Amount': np.random.exponential(50),
                'V1': np.random.normal(0, 1),
                'V2': np.random.normal(0, 1),
                'V3': np.random.normal(0, 1),
            })
        ref_df = pd.DataFrame(ref_data)
        
        # Load current production data
        prod_df = pd.read_csv('data/production_data.csv')
        
        # Perform KS tests for drift detection
        drift_results = {}
        features_to_check = ['Amount', 'V1', 'V2', 'V3']
        
        for feature in features_to_check:
            ks_stat, p_value = stats.ks_2samp(ref_df[feature], prod_df[feature])
            drift_detected = p_value < 0.05
            drift_results[feature] = {
                'ks_statistic': ks_stat,
                'p_value': p_value,
                'drift_detected': drift_detected
            }
            
            status = 'ðŸš¨ DRIFT DETECTED' if drift_detected else 'âœ… NO DRIFT'
            print(f'{feature}: {status} (KS={ks_stat:.3f}, p={p_value:.3f})')
        
        # Overall drift assessment
        total_drift = sum(1 for r in drift_results.values() if r['drift_detected'])
        drift_severity = 'HIGH' if total_drift >= 3 else 'MEDIUM' if total_drift >= 2 else 'LOW'
        
        print(f'\\nðŸ“Š Drift Summary: {total_drift}/{len(features_to_check)} features showing drift')
        print(f'ðŸŽ¯ Drift Severity: {drift_severity}')
        
        # Save drift report
        import json
        with open('drift_report.json', 'w') as f:
            json.dump({
                'total_features_checked': len(features_to_check),
                'features_with_drift': total_drift,
                'drift_severity': drift_severity,
                'detailed_results': drift_results
            }, f, indent=2)
        "

    - name: Model performance monitoring
      if: ${{ github.event.inputs.check_model_performance == 'true' || github.event.inputs.check_model_performance == '' }}
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        print('=== MODEL PERFORMANCE MONITORING ===')
        
        # Load production data
        df = pd.read_csv('data/production_data.csv')
        
        # Simulate model predictions (in real scenario, you'd load actual model)
        np.random.seed(42)
        n_samples = len(df)
        
        # Simulate realistic predictions with some performance degradation
        true_labels = df['Class'].values
        predicted_probs = np.random.beta(0.8, 4, n_samples)  # Skewed towards 0
        
        # Add some bias based on true labels to make it realistic
        for i in range(n_samples):
            if true_labels[i] == 1:  # True fraud
                predicted_probs[i] = max(predicted_probs[i], np.random.beta(2, 1))
        
        predictions = (predicted_probs > 0.5).astype(int)
        
        # Calculate metrics
        accuracy = accuracy_score(true_labels, predictions)
        precision = precision_score(true_labels, predictions, zero_division=0)
        recall = recall_score(true_labels, predictions, zero_division=0)
        f1 = f1_score(true_labels, predictions, zero_division=0)
        
        print(f'ðŸ“Š Current Model Performance:')
        print(f'   Accuracy:  {accuracy:.3f}')
        print(f'   Precision: {precision:.3f}')
        print(f'   Recall:    {recall:.3f}')
        print(f'   F1-Score:  {f1:.3f}')
        
        # Compare with baseline/expected performance
        baseline_metrics = {
            'accuracy': 0.995,
            'precision': 0.850,
            'recall': 0.750,
            'f1': 0.800
        }
        
        current_metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
        
        print(f'\\nðŸ“ˆ Performance vs Baseline:')
        performance_alerts = []
        
        for metric, current_val in current_metrics.items():
            baseline_val = baseline_metrics[metric]
            degradation = (baseline_val - current_val) / baseline_val * 100
            
            if degradation > 10:
                status = 'ðŸš¨ CRITICAL'
                performance_alerts.append(f'{metric}: {degradation:.1f}% degradation')
            elif degradation > 5:
                status = 'âš ï¸  WARNING'
                performance_alerts.append(f'{metric}: {degradation:.1f}% degradation')
            else:
                status = 'âœ… OK'
            
            print(f'   {metric.capitalize()}: {status} ({current_val:.3f} vs {baseline_val:.3f})')
        
        # Save performance report
        import json
        with open('performance_report.json', 'w') as f:
            json.dump({
                'current_metrics': current_metrics,
                'baseline_metrics': baseline_metrics,
                'alerts': performance_alerts,
                'samples_evaluated': n_samples
            }, f, indent=2)
        
        if performance_alerts:
            print(f'\\nðŸš¨ PERFORMANCE ALERTS: {len(performance_alerts)} issues detected')
            for alert in performance_alerts:
                print(f'   - {alert}')
        else:
            print(f'\\nâœ… Model performance within acceptable thresholds')
        "

    - name: Generate monitoring summary
      run: |
        python -c "
        import json
        from datetime import datetime
        
        print('\\n' + '='*50)
        print('MONITORING SUMMARY')
        print('='*50)
        
        # Load reports
        try:
            with open('drift_report.json', 'r') as f:
                drift_data = json.load(f)
            print(f'ðŸ” Data Drift: {drift_data[\"drift_severity\"]} ({drift_data[\"features_with_drift\"]}/{drift_data[\"total_features_checked\"]} features)')
        except:
            print('ðŸ” Data Drift: Not checked')
        
        try:
            with open('performance_report.json', 'r') as f:
                perf_data = json.load(f)
            alerts_count = len(perf_data['alerts'])
            print(f'ðŸ“Š Model Performance: {alerts_count} alerts detected')
            
            if alerts_count > 0:
                print('   Alerts:')
                for alert in perf_data['alerts']:
                    print(f'   - {alert}')
        except:
            print('ðŸ“Š Model Performance: Not checked')
        
        print(f'ðŸ“… Report generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")}')
        "

    - name: Upload monitoring reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: monitoring-reports-${{ github.run_id }}
        path: |
          data_quality_report.txt
          drift_report.json
          performance_report.json
        retention-days: 30