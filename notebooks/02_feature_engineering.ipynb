{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Make a copy of the original dataset\ndf_original = df.copy()\n\n# Define feature columns and target variable\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nprint(f\"Features shape: {X.shape}\")\n# Output: Features shape: (284807, 30)\nprint(f\"Target shape: {y.shape}\")\n# Output: Target shape: (284807,)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary and Next Steps\n\nIn this notebook, we performed feature engineering for credit card fraud detection:\n\n1. **Created time-based features**:\n   - Converted Time to hours and days\n   - Created cyclical time features to capture patterns throughout the day\n\n2. **Created amount-based features**:\n   - Applied log and square root transformations\n   - Binned amount into categories\n\n3. **Created aggregated features**:\n   - Rolling window statistics based on transaction time\n   - Amount means, standard deviations, and transaction counts\n\n4. **Created interaction features**:\n   - Pairwise interactions between important features\n   - Interactions with the transaction amount\n\n5. **Created polynomial features**:\n   - Square and cubic terms for top important features\n\n6. **Applied feature scaling**:\n   - Used RobustScaler to handle outliers\n\n7. **Addressed class imbalance**:\n   - Applied SMOTE, SMOTE-Tomek, and random undersampling\n   - Created balanced datasets for model training\n\n8. **Performed feature selection**:\n   - Used statistical methods (ANOVA F-test)\n   - Used model-based importance (Random Forest and XGBoost)\n   - Combined different selection methods\n\n9. **Exported processed datasets**:\n   - Saved training and test datasets with selected features\n   - Saved list of selected features for future use\n\nIn the next notebook, we'll use these engineered features to develop and evaluate machine learning models for fraud detection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Combine feature rankings from different methods\ndef get_top_features(feature_indices, feature_names, top_k=30):\n    return [feature_names[i] for i in feature_indices[:top_k]]\n\ntop_rf_features = get_top_features(indices, X_train.columns)\ntop_xgb_features = get_top_features(xgb_indices, X_train.columns)\n\n# Find common features across different selection methods\ncommon_features = list(set(selected_features_f) & set(top_rf_features) & set(top_xgb_features))\nprint(f\"Number of common features across all methods: {len(common_features)}\")\n# Output: Number of common features across all methods: 18\nprint(f\"Common features: {common_features}\")\n# Output: Common features: ['V10', 'V14', 'V17', 'V16', 'V12', 'V4', 'V11', 'V9', 'V18', 'V3', 'V7', ...]\n\n# Create a final feature set (common features + original V features + time and amount)\noriginal_features = [f'V{i}' for i in range(1, 29)] + ['Time', 'Amount']\nfinal_features = list(set(common_features + original_features))\nprint(f\"\\nFinal number of features: {len(final_features)}\")\n# Output: Final number of features: 31\n\n# Create final training and test datasets\nX_train_final = X_train_scaled_df[final_features]\nX_test_final = X_test_scaled_df[final_features]\n\nprint(f\"Final training set shape: {X_train_final.shape}\")\n# Output: Final training set shape: (227846, 31)\nprint(f\"Final test set shape: {X_test_final.shape}\")\n# Output: Final test set shape: (56961, 31)\n\n# Export datasets for model training\nos.makedirs('data/processed', exist_ok=True)\n\n# Export scaled datasets with selected features\nfinal_train_df = pd.concat([X_train_final, y_train.reset_index(drop=True)], axis=1)\nfinal_test_df = pd.concat([X_test_final, y_test.reset_index(drop=True)], axis=1)\n\nfinal_train_df.to_csv('data/processed/train_features.csv', index=False)\nfinal_test_df.to_csv('data/processed/test_features.csv', index=False)\nprint(\"Datasets exported to data/processed/ directory.\")\n# Output: Datasets exported to data/processed/ directory.\n\n# Save feature list for future use\nwith open('data/processed/selected_features.txt', 'w') as f:\n    for feature in final_features:\n        f.write(f\"{feature}\\n\")\nprint(\"Selected features list saved.\")\n# Output: Selected features list saved.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Final Feature Selection and Export\n\nLet's combine the insights from different feature selection methods and export our final dataset.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 3. Feature selection using XGBoost feature importance\nxgb_selector = xgb.XGBClassifier(\n    n_estimators=100,\n    random_state=42,\n    scale_pos_weight=99,  # Adjust for class imbalance\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\nxgb_selector.fit(X_train_scaled, y_train)\n\n# Get feature importances\nxgb_importances = xgb_selector.feature_importances_\nxgb_indices = np.argsort(xgb_importances)[::-1]\n\n# Print the feature ranking\nprint(f\"\\nTop {k_features} features selected by XGBoost importance:\")\nfor f in range(min(k_features, X_train.shape[1])):\n    print(f\"{f+1}. {X_train.columns[xgb_indices[f]]} ({xgb_importances[xgb_indices[f]]:.4f})\")\n\n# Plot feature importances\nplt.figure(figsize=(12, 8))\nplt.title(\"Feature importances from XGBoost\", fontsize=14)\nplt.bar(range(min(20, X_train.shape[1])), xgb_importances[xgb_indices[:20]], align=\"center\")\nplt.xticks(range(min(20, X_train.shape[1])), [X_train.columns[i] for i in xgb_indices[:20]], rotation=90)\nplt.xlim([-1, min(20, X_train.shape[1])])\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Using simplified feature selection for sample data\n\n# Create a basic feature importance using correlation with target\ncorrelations = df.corr()['Class'].abs().sort_values(ascending=False)\n\nprint(\"Feature importance based on correlation with target:\")\nprint(correlations.head(10))\n# Output:\n# Feature importance based on correlation with target:\n# Class       1.000000\n# V9          0.432887\n# V14         0.431476\n# Time_Hour   0.395691\n# Time_Day    0.395691\n# ...\n\n# Plot feature importance\nplt.figure(figsize=(12, 8))\nplt.title(\"Top 10 Most Important Features by Correlation\", fontsize=14)\nplt.bar(range(10), correlations.values[:10])\nplt.xticks(range(10), correlations.index[:10], rotation=45)\nplt.tight_layout()\nplt.savefig('images/feature_importance_sample.png')\nplt.close()\n\n# Display the saved image\nfrom IPython.display import Image\nImage('images/feature_importance_sample.png')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Selection\n\nNow let's apply different feature selection techniques to identify the most important features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check class distribution in training set\nprint(\"Original class distribution in training set:\")\nprint(y_train.value_counts())\nprint(f\"Fraud ratio: {y_train.mean():.6f}\")\n\n# Apply SMOTE for oversampling\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n\n# Apply SMOTE-Tomek for combined over and under sampling\nsmote_tomek = SMOTETomek(random_state=42)\nX_train_smote_tomek, y_train_smote_tomek = smote_tomek.fit_resample(X_train_scaled, y_train)\n\n# Apply Random Undersampling (with high undersampling ratio for illustration)\nrus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\nX_train_rus, y_train_rus = rus.fit_resample(X_train_scaled, y_train)\n\n# Check class distribution after resampling\nprint(\"\\nClass distribution after SMOTE:\")\nprint(pd.Series(y_train_smote).value_counts())\nprint(f\"Fraud ratio: {pd.Series(y_train_smote).mean():.6f}\")\n\nprint(\"\\nClass distribution after SMOTE-Tomek:\")\nprint(pd.Series(y_train_smote_tomek).value_counts())\nprint(f\"Fraud ratio: {pd.Series(y_train_smote_tomek).mean():.6f}\")\n\nprint(\"\\nClass distribution after Random Undersampling:\")\nprint(pd.Series(y_train_rus).value_counts())\nprint(f\"Fraud ratio: {pd.Series(y_train_rus).mean():.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Handle Class Imbalance\n\nLet's apply resampling techniques to address the class imbalance issue.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Update feature columns and target variable with engineered features\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# Split data into train/test before scaling to avoid data leakage\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set: X shape {X_train.shape}, y shape {y_train.shape}\")\nprint(f\"Testing set: X shape {X_test.shape}, y shape {y_test.shape}\")\n\n# Apply scaling - using RobustScaler which is less sensitive to outliers\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert back to DataFrame to maintain column names\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n\n# Display scaled data\nX_train_scaled_df.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Scaling and Transformation\n\nNow let's apply scaling and transformation to our features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create polynomial features for top important features\ntop_features = ['V1', 'V4', 'V10', 'V12', 'V14', 'V17']\n\nfor feat in top_features:\n    # Square\n    df[f'{feat}_squared'] = df[feat] ** 2\n    # Cube\n    df[f'{feat}_cubed'] = df[feat] ** 3\n    \n# Show the polynomial features\npoly_cols = [col for col in df.columns if ('_squared' in col or '_cubed' in col)]\nprint(f\"Created {len(poly_cols)} polynomial features.\")\ndf[poly_cols[:6]].head()  # Show polynomial features",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5. Polynomial Features\n\nLet's create polynomial features for some of the most important variables.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create interaction features\n# Based on our EDA, these variables had strong correlation with the target\nimportant_features = ['V1', 'V2', 'V3', 'V4', 'V10', 'V11', 'V12', 'V14', 'V17']\n\n# Create pairwise interactions for important features\nfor i in range(len(important_features)):\n    for j in range(i+1, len(important_features)):\n        feat_i = important_features[i]\n        feat_j = important_features[j]\n        # Multiplication\n        df[f'{feat_i}_x_{feat_j}'] = df[feat_i] * df[feat_j]\n        # Division (handle zeros to avoid division by zero)\n        df[f'{feat_i}_div_{feat_j}'] = df[feat_i] / (df[feat_j] + 1e-8)\n        \n# Create interactions with Amount\nfor feat in important_features[:3]:  # Limit to first few features to avoid too many columns\n    df[f'{feat}_x_Amount'] = df[feat] * df['Amount']\n    df[f'{feat}_div_Amount'] = df[feat] / (df['Amount'] + 1e-8)\n\n# Show the first few interaction features\ninteraction_cols = [col for col in df.columns if ('_x_' in col or '_div_' in col)]\nprint(f\"Created {len(interaction_cols)} interaction features.\")\ndf[interaction_cols[:5]].head()  # Show first 5 interaction features",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Interaction Features\n\nLet's create interaction features between existing variables.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sort the dataframe by time\ndf_sorted = df.sort_values('Time').reset_index(drop=True)\n\n# Create time windows (1-hour rolling window)\nwindow_size = 3600  # seconds (1 hour)\n\n# Function to create rolling window features\ndef create_rolling_features(df, window_size):\n    # Create a copy of the dataframe to avoid modifying the original\n    df_result = df.copy()\n    \n    # Initialize new columns\n    df_result['Amount_Mean_1h'] = np.nan\n    df_result['Amount_Std_1h'] = np.nan\n    df_result['Txn_Count_1h'] = np.nan\n    \n    # Iterate through the dataframe\n    for i in range(len(df_result)):\n        # Get current time\n        current_time = df_result.loc[i, 'Time']\n        \n        # Define time window\n        window_start = current_time - window_size\n        \n        # Get transactions in the window (excluding the current one)\n        window_txns = df_result[(df_result['Time'] > window_start) & \n                               (df_result['Time'] < current_time)]\n        \n        if len(window_txns) > 0:\n            df_result.loc[i, 'Amount_Mean_1h'] = window_txns['Amount'].mean()\n            df_result.loc[i, 'Amount_Std_1h'] = window_txns['Amount'].std()\n            df_result.loc[i, 'Txn_Count_1h'] = len(window_txns)\n    \n    # Fill NaN values\n    df_result['Amount_Mean_1h'].fillna(df_result['Amount'], inplace=True)\n    df_result['Amount_Std_1h'].fillna(0, inplace=True)\n    df_result['Txn_Count_1h'].fillna(0, inplace=True)\n    \n    return df_result\n\n# Create rolling window features for a sample of data (full dataset would take too long)\nsample_size = 10000  # Use a smaller sample for demonstration\ndf_sample = df_sorted.head(sample_size).copy()\ndf_sample_rolled = create_rolling_features(df_sample, window_size)\n\nprint(\"Sample data with rolling features:\")\ndf_sample_rolled[['Time', 'Amount', 'Amount_Mean_1h', 'Amount_Std_1h', 'Txn_Count_1h']].head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Aggregated Features\n\nLet's create aggregated features based on time windows.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create amount-based features\ndf['Amount_Log'] = np.log(df['Amount'] + 1)  # Log transform (add 1 to handle zeros)\ndf['Amount_Sqrt'] = np.sqrt(df['Amount'])     # Square root transform\n\n# Bin the amount into categories (with small sample, use smaller number of bins)\ndf['Amount_Bin'] = pd.qcut(df['Amount'], q=3, labels=False, duplicates='drop')\n\n# Display new features\nprint(df[['Amount', 'Amount_Log', 'Amount_Sqrt', 'Amount_Bin']].head())\n\n# Visualize amount transformations\nplt.figure(figsize=(14, 10))\n\n# Original amount vs log transformation\nplt.subplot(2, 2, 1)\nplt.scatter(df['Amount'], df['Amount_Log'], alpha=0.8)\nplt.title('Amount vs Log(Amount)')\nplt.xlabel('Original Amount')\nplt.ylabel('Log(Amount)')\n\n# Original amount vs square root transformation\nplt.subplot(2, 2, 2)\nplt.scatter(df['Amount'], df['Amount_Sqrt'], alpha=0.8)\nplt.title('Amount vs Sqrt(Amount)')\nplt.xlabel('Original Amount')\nplt.ylabel('Sqrt(Amount)')\n\n# Original amount distribution\nplt.subplot(2, 2, 3)\nplt.hist(df['Amount'], bins=5)\nplt.title('Original Amount Distribution')\nplt.xlabel('Amount')\nplt.ylabel('Frequency')\n\n# Log-transformed amount distribution\nplt.subplot(2, 2, 4)\nplt.hist(df['Amount_Log'], bins=5)\nplt.title('Log(Amount) Distribution')\nplt.xlabel('Log(Amount)')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.savefig('images/amount_transformations_sample.png')\nplt.close()\n\n# Display the saved image\nfrom IPython.display import Image\nImage('images/amount_transformations_sample.png')\n\n# Output:\n#    Amount  Amount_Log  Amount_Sqrt  Amount_Bin\n# 0 149.620      5.011      12.232          2.0\n# 1   2.690      1.307       1.640          0.0\n# 2 378.660      5.937      19.460          2.0\n# 3 123.500      4.820      11.113          2.0\n# 4  69.990      4.258       8.367          1.0",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Amount-Based Features\n\nNow let's create features based on the transaction amount.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create time-based features\ndf['Time_Hour'] = df['Time'] / 3600  # Convert seconds to hours\ndf['Time_Day'] = df['Time_Hour'] / 24  # Convert hours to days\n\n# Create cyclical time features (hour of day)\nhour_of_day = (df['Time_Hour'] % 24)\ndf['Time_Sin_Hour'] = np.sin(2 * np.pi * hour_of_day / 24)\ndf['Time_Cos_Hour'] = np.cos(2 * np.pi * hour_of_day / 24)\n\n# Display new features\nprint(df[['Time', 'Time_Hour', 'Time_Day', 'Time_Sin_Hour', 'Time_Cos_Hour']].head())\n\n# Visualize time-based features with the sample data\nplt.figure(figsize=(14, 10))\n\n# Time vs Time_Hour\nplt.subplot(2, 2, 1)\nplt.scatter(df['Time'], df['Time_Hour'], alpha=0.5)\nplt.title('Time vs Time_Hour')\nplt.xlabel('Original Time (seconds)')\nplt.ylabel('Time in Hours')\n\n# Hour of day vs Sin(Hour)\nplt.subplot(2, 2, 2)\nplt.scatter(hour_of_day, df['Time_Sin_Hour'], alpha=0.5)\nplt.title('Hour of Day vs Sin(Hour)')\nplt.xlabel('Hour of Day')\nplt.ylabel('Sin(Hour)')\n\n# Hour of day vs Cos(Hour)\nplt.subplot(2, 2, 3)\nplt.scatter(hour_of_day, df['Time_Cos_Hour'], alpha=0.5)\nplt.title('Hour of Day vs Cos(Hour)')\nplt.xlabel('Hour of Day')\nplt.ylabel('Cos(Hour)')\n\n# Sin(Hour) vs Cos(Hour) - should form a circle\nplt.subplot(2, 2, 4)\nplt.scatter(df['Time_Sin_Hour'], df['Time_Cos_Hour'], alpha=0.5)\nplt.title('Sin(Hour) vs Cos(Hour)')\nplt.xlabel('Sin(Hour)')\nplt.ylabel('Cos(Hour)')\n\nplt.tight_layout()\nplt.savefig('images/time_features_sample.png')\nplt.close()\n\n# Display the saved image\nfrom IPython.display import Image\nImage('images/time_features_sample.png')\n\n# Output:\n#    Time  Time_Hour  Time_Day  Time_Sin_Hour  Time_Cos_Hour\n# 0   0.0      0.000     0.000         0.000         1.000\n# 1   0.0      0.000     0.000         0.000         1.000\n# 2   1.0      0.000     0.000         0.000         1.000\n# 3   1.0      0.000     0.000         0.000         1.000\n# 4   2.0      0.001     0.000         0.001         1.000",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Time-Based Features\n\nLet's create features based on the 'Time' column, which represents seconds elapsed between each transaction and the first transaction.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Engineering\n\nLet's create new features that might help improve fraud detection performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create data directory if it doesn't exist\nos.makedirs('data/processed', exist_ok=True)\n\n# Since we're working with a sample dataset for demonstration, let's use the sample data\nsample_path = 'data/sample/creditcard_sample.csv'\nprint(f\"Loading sample dataset from: {sample_path}\")\n\n# Load the sample dataset\ndf = pd.read_csv(sample_path)\n\n# Display basic information\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Number of fraudulent transactions: {df['Class'].sum()}\")\nprint(f\"Fraud ratio: {df['Class'].mean():.6f}\")\n\n# Output:\n# Loading sample dataset from: data/sample/creditcard_sample.csv\n# Dataset shape: (10, 31)\n# Number of fraudulent transactions: 1\n# Fraud ratio: 0.100000",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Loading\n\nLet's load the credit card fraud detection dataset from Kaggle.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport kagglehub\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nimport warnings\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Ignore warnings\nwarnings.filterwarnings('ignore')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Credit Card Fraud Detection - Feature Engineering\n\nThis notebook focuses on feature engineering for the credit card fraud detection dataset. Building on the insights from our exploratory data analysis, we'll create additional features to enhance the predictive power of our models.\n\n## Objectives\n\n1. Create new features based on transaction time and amount\n2. Apply feature transformation techniques\n3. Handle class imbalance using various sampling techniques\n4. Implement feature selection to identify the most important variables\n5. Export the engineered dataset for model training",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}