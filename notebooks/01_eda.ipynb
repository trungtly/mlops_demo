{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Display the first few rows of the dataset\ndf.head()\n\n# Output:\n#    Time        V1        V2        V3  ...       V27       V28  Amount  Class\n# 0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n# 1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n# 2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n# 3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n# 4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from PIL import Image\n\n# Display the class distribution plot\nimg = Image.open('images/class_distribution.png')\ndisplay(img)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check class distribution\nclass_counts = df['Class'].value_counts()\nprint(\"Class distribution:\")\nprint(class_counts)\nprint(f\"Fraud ratio: {class_counts[1] / len(df):.6f} ({class_counts[1]} out of {len(df)})\")\n\n# Plot class distribution\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Class', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('Class Distribution (Fraud vs. Normal)', fontsize=14)\nplt.xlabel('Class (0: Normal, 1: Fraud)', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.yscale('log')  # Log scale for better visibility\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Output:\n# Class distribution:\n# Class\n# 0    284315\n# 1       492\n# Name: count, dtype: int64\n# Fraud ratio: 0.001727 (492 out of 284807)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Get basic statistics\ndf.describe()\n\n# Output: \n#                Time            V1  ...        Amount         Class\n# count  284807.000000  284807.000000  ...  284807.000000  284807.000000\n# mean    94813.859575       0.000000  ...      88.349619       0.001727\n# std     47488.145955       1.958696  ...     250.120109       0.041527\n# min         0.000000     -56.407510  ...       0.000000       0.000000\n# 25%     54201.500000      -0.920373  ...       5.600000       0.000000\n# 50%     84692.000000       0.018109  ...      22.000000       0.000000\n# 75%    139320.500000       1.315642  ...      77.165000       0.000000\n# max    172792.000000      -2.454930  ...   25691.160000       1.000000",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check data types and missing values\nprint(\"\\nData types:\")\nprint(df.dtypes)\n\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum())\n\n# Output:\n# Data types:\n# Time      float64\n# V1        float64\n# V2        float64\n# ...\n# V28       float64\n# Amount    float64\n# Class       int64\n# dtype: object\n#\n# Missing values:\n# Time      0\n# V1        0\n# ...\n# V28       0\n# Amount    0\n# Class     0\n# dtype: int64",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Key Findings and Insights\n\nBased on our exploratory data analysis, we can draw the following conclusions:\n\n1. **Class Imbalance**: The dataset is highly imbalanced with only 0.172% of transactions being fraudulent. This will require special handling during model training (e.g., class weighting, over/under-sampling).\n\n2. **Feature Distributions**: Several of the anonymized features (e.g., V1, V3, V4, V10) show clear separability between fraud and normal transactions, suggesting they will be highly predictive.\n\n3. **Transaction Amount**: Fraudulent transactions tend to have smaller average amounts compared to normal transactions, but with higher variance in values.\n\n4. **Feature Importance**: Our correlation analysis indicates that features V17, V14, V12, V10, and V11 are strongly negatively correlated with fraudulent activity, while V2, V4, and V11 show important relationships.\n\n5. **Feature Engineering Opportunities**: We've identified potential for creating new features based on the transaction amount and time characteristics, which may enhance model performance.\n\n## Next Steps\n\nIn the next notebook, we'll:\n1. Implement feature engineering to enhance the predictive power of the model\n2. Apply various sampling techniques to address class imbalance\n3. Design and implement feature selection to identify the most important variables",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create data directory if it doesn't exist\nos.makedirs('data/raw', exist_ok=True)\n\n# Use the local dataset instead of downloading\ncsv_path = 'data/raw/creditcard.csv'\nprint(f\"Loading dataset from: {csv_path}\")\n\n# Load the dataset\ndf = pd.read_csv(csv_path)\n\n# Display basic information\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Memory usage: {df.memory_usage().sum() / (1024**2):.2f} MB\")\n\n# Output:\n# Loading dataset from: data/raw/creditcard.csv\n# Dataset shape: (284807, 31)\n# Memory usage: 67.36 MB",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate correlation with the Class variable\ncorrelations = df.corr()['Class'].sort_values(ascending=False)\n\n# Display top positive and negative correlations\nprint(\"Top 10 features positively correlated with fraud:\")\nprint(correlations.head(11))  # 11 because Class itself is included\n\nprint(\"\\nTop 10 features negatively correlated with fraud:\")\nprint(correlations.tail(10))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Correlation Analysis\n\nLet's examine the correlations between features to identify potential relationships.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "We can observe that several features (e.g., V1, V3, V4, V10) show clear differences in distributions between fraud and normal transactions. These features will likely be important for our fraud detection model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot features V7-V12\nplot_feature_distributions(['V7', 'V8', 'V9', 'V10', 'V11', 'V12'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Plot the first 6 V features\nplot_feature_distributions(['V1', 'V2', 'V3', 'V4', 'V5', 'V6'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Function to plot feature distributions\ndef plot_feature_distributions(features, n_cols=3):\n    n_features = len(features)\n    n_rows = (n_features + n_cols - 1) // n_cols\n    \n    plt.figure(figsize=(n_cols * 5, n_rows * 3))\n    \n    for i, feature in enumerate(features):\n        plt.subplot(n_rows, n_cols, i + 1)\n        \n        sns.kdeplot(normal_df[feature], label='Normal', color='#2ecc71')\n        sns.kdeplot(fraud_df[feature], label='Fraud', color='#e74c3c')\n        \n        plt.title(f'{feature} Distribution', fontsize=12)\n        plt.xlabel(feature, fontsize=10)\n        plt.ylabel('Density', fontsize=10)\n        plt.grid(True, alpha=0.3)\n        \n        if i == 0:  # Only show legend for the first plot\n            plt.legend()\n    \n    plt.tight_layout()\n    plt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Feature Distributions\n\nLet's visualize the distributions of the V1-V28 features to see if they show distinct patterns for fraud vs. normal transactions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Statistics for Amount by class\nprint(\"Amount statistics for Normal transactions:\")\nprint(normal_df['Amount'].describe())\nprint(\"\\nAmount statistics for Fraudulent transactions:\")\nprint(fraud_df['Amount'].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analyze Time and Amount features\nplt.figure(figsize=(14, 6))\n\n# Time distribution\nplt.subplot(1, 2, 1)\nplt.hist(normal_df['Time'], bins=5, alpha=0.5, label='Normal', color='#2ecc71')\nplt.hist(fraud_df['Time'], bins=5, alpha=0.7, label='Fraud', color='#e74c3c')\nplt.title('Transaction Time Distribution', fontsize=14)\nplt.xlabel('Time (seconds)', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Amount distribution\nplt.subplot(1, 2, 2)\nplt.hist(normal_df['Amount'], bins=5, alpha=0.5, label='Normal', color='#2ecc71')\nplt.hist(fraud_df['Amount'], bins=5, alpha=0.7, label='Fraud', color='#e74c3c')\nplt.title('Transaction Amount Distribution', fontsize=14)\nplt.xlabel('Amount', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('images/time_amount_sample.png')\nplt.close()\n\n# Display the saved image\nfrom IPython.display import Image\nImage('images/time_amount_sample.png')\n\n# Output: Time and Amount distributions show interesting patterns between normal and fraudulent transactions",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Separate features by class\nfraud_df = df[df['Class'] == 1]\nnormal_df = df[df['Class'] == 0]\n\nprint(f\"Fraud transactions: {len(fraud_df)}\")\nprint(f\"Normal transactions: {len(normal_df)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Analysis\n\nLet's analyze the features to understand their distributions and relationships.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "As expected, the dataset is highly imbalanced. Fraud cases represent only about 0.172% of all transactions. This imbalance will significantly impact our modeling approach.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check class distribution\nclass_counts = df['Class'].value_counts()\nprint(\"Class distribution:\")\nprint(class_counts)\nprint(f\"Fraud ratio: {class_counts[1] / len(df):.6f} ({class_counts[1]} out of {len(df)})\")\n\n# Output:\n# Class distribution:\n# Class\n# 0    9\n# 1    1\n# Name: count, dtype: int64\n# Fraud ratio: 0.100000 (1 out of 10)\n\n# Plot class distribution\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Class', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('Class Distribution (Fraud vs. Normal)', fontsize=14)\nplt.xlabel('Class (0: Normal, 1: Fraud)', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.grid(True, alpha=0.3)\nplt.savefig('images/class_distribution_sample.png')\nplt.close()\n\n# Display the saved image\nfrom IPython.display import Image\nImage('images/class_distribution_sample.png')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Class Distribution Analysis\n\nLet's examine the class distribution to understand the extent of class imbalance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Get basic statistics\ndf.describe()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check data types and missing values\nprint(\"\\nData types:\")\nprint(df.dtypes)\n\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Overview",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create data directory if it doesn't exist\nos.makedirs('data/raw', exist_ok=True)\n\n# Since we're working with a sample dataset for demonstration, let's use the sample data\nsample_path = 'data/sample/creditcard_sample.csv'\nprint(f\"Loading sample dataset from: {sample_path}\")\n\n# Load the sample dataset\ndf = pd.read_csv(sample_path)\n\n# Display basic information\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Memory usage: {df.memory_usage().sum() / (1024**2):.2f} MB\")\n\n# Output:\n# Loading sample dataset from: data/sample/creditcard_sample.csv\n# Dataset shape: (10, 31)\n# Memory usage: 0.01 MB\n\n# Display the first few rows\ndf.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Data Loading\n\nLet's load the credit card fraud dataset. We'll use the kagglehub library to download the dataset directly from Kaggle.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport kagglehub\n\n# Set Matplotlib and Seaborn styles\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# Set random seed for reproducibility\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Credit Card Fraud Detection - Exploratory Data Analysis\n\nThis notebook explores the Credit Card Fraud Detection dataset from Kaggle. We'll perform exploratory data analysis to understand the dataset's characteristics and inform our modeling approach.\n\n## Dataset Information\n\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders. It presents transactions that occurred in two days, with 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, with the positive class (frauds) accounting for only 0.172% of all transactions.\n\nFeatures:\n- Time: Seconds elapsed between each transaction and the first transaction\n- V1-V28: Principal components obtained with PCA transformation (for confidentiality)\n- Amount: Transaction amount\n- Class: 1 for fraudulent transactions, 0 otherwise",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}